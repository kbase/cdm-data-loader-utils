import click
from refseq_pipeline.core.spark_delta import build_spark
from refseq_pipeline.core.datasets_api import fetch_reports_by_taxon
from refseq_pipeline.core.cdm_parse import parse_reports
from refseq_pipeline.core.driver import process_and_write_reports


"""

#####  PASS  #####
example usage:
data-dir can change to your desired path

     python -m refseq_pipeline.cli.debug_parse_one_taxon \
    --taxid 752788 \
    --database refseq_api \
    --table assembly_stats \
    --mode overwrite \
    --data-dir /Users/yuewang/Documents/RefSeq/delta_data \
    --preview-only

 """


@click.command()
@click.option("--taxid", required=True, help="NCBI Taxonomy ID to fetch.")
@click.option("--database", default="refseq_api", show_default=True)
@click.option("--table", default="assembly_stats", show_default=True)
@click.option("--mode", default="overwrite", type=click.Choice(["append", "overwrite"]))
@click.option("--data-dir", default=None, help="Optional external delta path.")
@click.option("--preview-only", is_flag=True, help="Only show parsed results, don't write.")


def main(taxid, database, table, mode, data_dir, preview_only):
    """
    Debug parser for a single TaxID.
    
    The function is designed to:
    - Fetch taxonomic genome reports from NCBI for a specific TaxID
    - Parse them into a structured Spark DataFrame using CDM schema
    - Optionally preview the parsed result in the console
    - Optionally write the parsed result to a Delta Lake table
    
    """

    # Initialize Spark Session
    spark = build_spark(database)
    print(f"[DEBUG] Fetching reports for TaxID: {taxid}")
    
    # Fetch NCBI genome reports for this taxon
    reports = list(fetch_reports_by_taxon(taxon=taxid, debug=True))
    if not reports:
        print("[DEBUG] No reports found.")
        return
    
    # Parse the reports into Spark DataFrame using CDM structure
    df = parse_reports(reports, return_spark=True, spark=spark)
    print(f"[DEBUG] Parsed {df.count()} CDM rows for TaxID={taxid}")

    # Preview the parsed DataFrame â€” convert to string and fill nulls
    df.select([df[c].cast("string").alias(c) for c in df.columns]) \
        .na.fill("None") \
        .show(truncate=False)
    
    # Write parsed data to Delta Lake 
    if not preview_only:
        print(f"[debug] Writing to Delta table: {database}.{table}")
        process_and_write_reports(
            spark=spark,
            reports=reports,
            database=database,
            table=table,
            mode=mode,
            data_dir=data_dir,
        )

if __name__ == "__main__":
    main()


